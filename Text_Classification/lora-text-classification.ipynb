{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[],"dockerImageVersionId":30746,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2024-07-24T09:50:59.276855Z","iopub.execute_input":"2024-07-24T09:50:59.277174Z","iopub.status.idle":"2024-07-24T09:51:00.035665Z","shell.execute_reply.started":"2024-07-24T09:50:59.277150Z","shell.execute_reply":"2024-07-24T09:51:00.034571Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"code","source":"!pip install --q peft evaluate","metadata":{"execution":{"iopub.status.busy":"2024-07-24T10:28:43.438149Z","iopub.execute_input":"2024-07-24T10:28:43.438472Z","iopub.status.idle":"2024-07-24T10:28:58.290699Z","shell.execute_reply.started":"2024-07-24T10:28:43.438442Z","shell.execute_reply":"2024-07-24T10:28:58.289462Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"code","source":"from datasets import load_dataset, DatasetDict, Dataset\n\nfrom transformers import (\n    AutoTokenizer, AutoConfig, AutoModelForSequenceClassification,\n    DataCollatorWithPadding, TrainingArguments, Trainer\n)\n\nfrom peft import PeftModel, PeftConfig, get_peft_model, LoraConfig\nimport evaluate\nimport torch\nimport numpy as np","metadata":{"execution":{"iopub.status.busy":"2024-07-24T10:29:04.852448Z","iopub.execute_input":"2024-07-24T10:29:04.853180Z","iopub.status.idle":"2024-07-24T10:29:24.355368Z","shell.execute_reply.started":"2024-07-24T10:29:04.853142Z","shell.execute_reply":"2024-07-24T10:29:24.354341Z"},"trusted":true},"execution_count":2,"outputs":[{"name":"stderr","text":"2024-07-24 10:29:12.385591: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n2024-07-24 10:29:12.385712: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n2024-07-24 10:29:12.503975: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","output_type":"stream"}]},{"cell_type":"markdown","source":"**<h1>Base model**","metadata":{}},{"cell_type":"code","source":"model_checkpoint = 'distilbert-base-uncased'\n\n# define label maps\nid2label = {0: \"sadness\", 1: \"joy\", 2: \"love\", 3: \"anger\", 4: \"fear\", 5: \"surprise\"}\nlabel2id = {\"sadness\": 0, \"joy\": 1, \"love\": 2, \"anger\": 3, \"fear\": 4, \"surprise\": 5}\n\n# generate classification model from model_checkpoint\nmodel = AutoModelForSequenceClassification.from_pretrained(model_checkpoint, num_labels=6, id2label=id2label, label2id=label2id)","metadata":{"execution":{"iopub.status.busy":"2024-07-24T10:29:27.739396Z","iopub.execute_input":"2024-07-24T10:29:27.740578Z","iopub.status.idle":"2024-07-24T10:29:36.261902Z","shell.execute_reply.started":"2024-07-24T10:29:27.740543Z","shell.execute_reply":"2024-07-24T10:29:36.261134Z"},"trusted":true},"execution_count":3,"outputs":[{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/483 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b65ded0efc9e4af291e4f58a78cf01a2"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/268M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"3b0e4d8adea94b53b6a37da1a8b6282d"}},"metadata":{}},{"name":"stderr","text":"Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","output_type":"stream"}]},{"cell_type":"markdown","source":"**<h1>Load data**","metadata":{}},{"cell_type":"code","source":"dataset = load_dataset('SetFit/emotion')","metadata":{"execution":{"iopub.status.busy":"2024-07-24T10:29:39.878732Z","iopub.execute_input":"2024-07-24T10:29:39.879086Z","iopub.status.idle":"2024-07-24T10:29:46.607865Z","shell.execute_reply.started":"2024-07-24T10:29:39.879056Z","shell.execute_reply":"2024-07-24T10:29:46.606942Z"},"trusted":true},"execution_count":4,"outputs":[{"output_type":"display_data","data":{"text/plain":"Downloading readme:   0%|          | 0.00/194 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e4c5ddb78fea456eab3457927eab8f2b"}},"metadata":{}},{"name":"stderr","text":"Repo card metadata block was not found. Setting CardData to empty.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Downloading data:   0%|          | 0.00/2.23M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d4c8c3a01b094e5eae58fa818776fb89"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading data:   0%|          | 0.00/276k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ddb28b71175b4acaba1835cd0594c954"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading data:   0%|          | 0.00/279k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f6581013b2a54f57a57693f49e9305a2"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating train split:   0%|          | 0/16000 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"6ee09d295eed47248c63b1f55fe32450"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating validation split:   0%|          | 0/2000 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ec14f7449bbe44e28690c66b7db47833"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating test split:   0%|          | 0/2000 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"6d15dc2c3d8b4e6292ff57839b0ff0b8"}},"metadata":{}}]},{"cell_type":"code","source":"dataset","metadata":{"execution":{"iopub.status.busy":"2024-07-24T10:29:49.547724Z","iopub.execute_input":"2024-07-24T10:29:49.548551Z","iopub.status.idle":"2024-07-24T10:29:49.554744Z","shell.execute_reply.started":"2024-07-24T10:29:49.548516Z","shell.execute_reply":"2024-07-24T10:29:49.553881Z"},"trusted":true},"execution_count":5,"outputs":[{"execution_count":5,"output_type":"execute_result","data":{"text/plain":"DatasetDict({\n    train: Dataset({\n        features: ['text', 'label', 'label_text'],\n        num_rows: 16000\n    })\n    validation: Dataset({\n        features: ['text', 'label', 'label_text'],\n        num_rows: 2000\n    })\n    test: Dataset({\n        features: ['text', 'label', 'label_text'],\n        num_rows: 2000\n    })\n})"},"metadata":{}}]},{"cell_type":"markdown","source":"**<h1>Preprocess data**","metadata":{}},{"cell_type":"code","source":"tokenizer = AutoTokenizer.from_pretrained(model_checkpoint, add_prefix_space=True)","metadata":{"execution":{"iopub.status.busy":"2024-07-24T10:29:53.701332Z","iopub.execute_input":"2024-07-24T10:29:53.702157Z","iopub.status.idle":"2024-07-24T10:29:55.777152Z","shell.execute_reply.started":"2024-07-24T10:29:53.702116Z","shell.execute_reply":"2024-07-24T10:29:55.775982Z"},"trusted":true},"execution_count":6,"outputs":[{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/48.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"61f5c8ea1a0c4bb18f802ac0b504f9c5"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"180a20d8c50248ef8f5097551705189a"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"3ca5a0e6f0f34f37869402f0e669da37"}},"metadata":{}}]},{"cell_type":"code","source":"def tokenize_function(examples):\n    text = examples['text']\n    \n    tokenized_inputs = tokenizer(text, truncation=True, return_tensors='np', max_length=512)\n    \n    return tokenized_inputs","metadata":{"execution":{"iopub.status.busy":"2024-07-24T10:29:58.062766Z","iopub.execute_input":"2024-07-24T10:29:58.063127Z","iopub.status.idle":"2024-07-24T10:29:58.069981Z","shell.execute_reply.started":"2024-07-24T10:29:58.063081Z","shell.execute_reply":"2024-07-24T10:29:58.069018Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"code","source":"# add pad token if none exists\nif tokenizer.pad_token is None:\n  tokenizer.add_special_tokens({'pad_token': '[PAD]'})\n  model.resize_token_embeddings(len(tokenizer))","metadata":{"execution":{"iopub.status.busy":"2024-07-24T10:30:03.527896Z","iopub.execute_input":"2024-07-24T10:30:03.528235Z","iopub.status.idle":"2024-07-24T10:30:03.532772Z","shell.execute_reply.started":"2024-07-24T10:30:03.528209Z","shell.execute_reply":"2024-07-24T10:30:03.531814Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"code","source":"# tokenize training and validation datasets\ntokenized_dataset = dataset.map(tokenize_function, batched=True)","metadata":{"execution":{"iopub.status.busy":"2024-07-24T10:30:06.270532Z","iopub.execute_input":"2024-07-24T10:30:06.271350Z","iopub.status.idle":"2024-07-24T10:30:07.548446Z","shell.execute_reply.started":"2024-07-24T10:30:06.271318Z","shell.execute_reply":"2024-07-24T10:30:07.547598Z"},"trusted":true},"execution_count":9,"outputs":[{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/16000 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b413bc7aef844bb6a0791dc34988732a"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/2000 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"79fbea8ce77349dd8d23e5e1eb85c9fe"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/2000 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f6a4ed6c7383484e9f9f46ae2d951eaa"}},"metadata":{}}]},{"cell_type":"code","source":"tokenized_dataset","metadata":{"execution":{"iopub.status.busy":"2024-07-24T10:30:10.042345Z","iopub.execute_input":"2024-07-24T10:30:10.043105Z","iopub.status.idle":"2024-07-24T10:30:10.048950Z","shell.execute_reply.started":"2024-07-24T10:30:10.043072Z","shell.execute_reply":"2024-07-24T10:30:10.048025Z"},"trusted":true},"execution_count":10,"outputs":[{"execution_count":10,"output_type":"execute_result","data":{"text/plain":"DatasetDict({\n    train: Dataset({\n        features: ['text', 'label', 'label_text', 'input_ids', 'attention_mask'],\n        num_rows: 16000\n    })\n    validation: Dataset({\n        features: ['text', 'label', 'label_text', 'input_ids', 'attention_mask'],\n        num_rows: 2000\n    })\n    test: Dataset({\n        features: ['text', 'label', 'label_text', 'input_ids', 'attention_mask'],\n        num_rows: 2000\n    })\n})"},"metadata":{}}]},{"cell_type":"code","source":"# create data collator\ndata_collator = DataCollatorWithPadding(tokenizer=tokenizer)","metadata":{"execution":{"iopub.status.busy":"2024-07-24T10:30:13.088850Z","iopub.execute_input":"2024-07-24T10:30:13.089681Z","iopub.status.idle":"2024-07-24T10:30:13.093455Z","shell.execute_reply.started":"2024-07-24T10:30:13.089637Z","shell.execute_reply":"2024-07-24T10:30:13.092559Z"},"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"markdown","source":"**<h1>Evaluation metrics**","metadata":{}},{"cell_type":"code","source":"# import accuracy evaluation metric\naccuracy = evaluate.load('accuracy')\n\n# define an evaluation function to pass into trainer later\ndef compute_metrics(p):\n  predictions, labels = p\n  predictions = np.argmax(predictions, axis=1)\n\n  return {\"accuracy\": accuracy.compute(predictions=predictions, references=labels)}","metadata":{"execution":{"iopub.status.busy":"2024-07-24T10:30:16.120560Z","iopub.execute_input":"2024-07-24T10:30:16.121151Z","iopub.status.idle":"2024-07-24T10:30:17.442075Z","shell.execute_reply.started":"2024-07-24T10:30:16.121119Z","shell.execute_reply":"2024-07-24T10:30:17.441289Z"},"trusted":true},"execution_count":12,"outputs":[{"output_type":"display_data","data":{"text/plain":"Downloading builder script:   0%|          | 0.00/4.20k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"132555c5adfc44debd16761784d53709"}},"metadata":{}}]},{"cell_type":"markdown","source":"**<h2>Untrained model performance**","metadata":{}},{"cell_type":"code","source":"text_list = [\"i am sad.\", \"today, i have a boy friend, i'm happy\", \"this is the most yummy dishes\",\n            \"wowwww\", \"exciting film!\"]\n\nprint(\"Utrained model predictions: \")\nfor text in text_list:\n    inputs = tokenizer.encode(text, return_tensors='pt')\n    logits = model(inputs).logits\n    predictions = torch.argmax(logits, dim=1).item()\n    \n    print(text + \" - \" + id2label[predictions])","metadata":{"execution":{"iopub.status.busy":"2024-07-24T10:30:21.532441Z","iopub.execute_input":"2024-07-24T10:30:21.533282Z","iopub.status.idle":"2024-07-24T10:30:21.828287Z","shell.execute_reply.started":"2024-07-24T10:30:21.533241Z","shell.execute_reply":"2024-07-24T10:30:21.827186Z"},"trusted":true},"execution_count":13,"outputs":[{"name":"stdout","text":"Utrained model predictions: \ni am sad. - fear\ntoday, i have a boy friend, i'm happy - fear\nthis is the most yummy dishes - fear\nwowwww - sadness\nexciting film! - fear\n","output_type":"stream"}]},{"cell_type":"markdown","source":"**<h1>Fine-tuning with LoRA**","metadata":{}},{"cell_type":"code","source":"peft_config = LoraConfig(task_type=\"SEQ_CLS\", # sequence classification\n                         r=4, # intrinsic rank of trainable weight matrix\n                         lora_alpha=32, # this is like a learning rate\n                         lora_dropout=0.01, # probablity of dropout\n                         target_modules=['q_lin']) # we apply lora to query layer","metadata":{"execution":{"iopub.status.busy":"2024-07-24T10:30:27.734116Z","iopub.execute_input":"2024-07-24T10:30:27.734456Z","iopub.status.idle":"2024-07-24T10:30:27.739493Z","shell.execute_reply.started":"2024-07-24T10:30:27.734429Z","shell.execute_reply":"2024-07-24T10:30:27.738471Z"},"trusted":true},"execution_count":14,"outputs":[]},{"cell_type":"code","source":"model = get_peft_model(model, peft_config)\nmodel.print_trainable_parameters()","metadata":{"execution":{"iopub.status.busy":"2024-07-24T10:30:31.445968Z","iopub.execute_input":"2024-07-24T10:30:31.446319Z","iopub.status.idle":"2024-07-24T10:30:31.468807Z","shell.execute_reply.started":"2024-07-24T10:30:31.446291Z","shell.execute_reply":"2024-07-24T10:30:31.467783Z"},"trusted":true},"execution_count":15,"outputs":[{"name":"stdout","text":"trainable params: 632,070 || all params: 67,590,156 || trainable%: 0.9352\n","output_type":"stream"}]},{"cell_type":"code","source":"# hyperparameters\nlr = 1e-3\nbatch_size = 4\nnum_epochs = 10\n\n# define training arguments\ntraining_args = TrainingArguments(\n    output_dir=model_checkpoint + '-lora-text-classification',\n    learning_rate=lr,\n    per_device_train_batch_size=batch_size,\n    per_device_eval_batch_size=batch_size,\n    num_train_epochs=num_epochs,\n    weight_decay=0.01,\n    evaluation_strategy='epoch',\n    save_strategy='epoch',\n    load_best_model_at_end=True,\n)","metadata":{"execution":{"iopub.status.busy":"2024-07-24T10:30:34.810739Z","iopub.execute_input":"2024-07-24T10:30:34.811110Z","iopub.status.idle":"2024-07-24T10:30:34.869083Z","shell.execute_reply.started":"2024-07-24T10:30:34.811082Z","shell.execute_reply":"2024-07-24T10:30:34.868098Z"},"trusted":true},"execution_count":16,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/transformers/training_args.py:1494: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ü§ó Transformers. Use `eval_strategy` instead\n  warnings.warn(\n","output_type":"stream"}]},{"cell_type":"code","source":"# creater trainer object\ntrainer = Trainer(\n    model=model,\n    args=training_args,\n    train_dataset=tokenized_dataset['train'],\n    eval_dataset=tokenized_dataset['validation'],\n    tokenizer=tokenizer,\n    data_collator=data_collator,\n    compute_metrics=compute_metrics,\n)\n\ntrainer.train()","metadata":{"execution":{"iopub.status.busy":"2024-07-24T10:30:41.994340Z","iopub.execute_input":"2024-07-24T10:30:41.994942Z","iopub.status.idle":"2024-07-24T10:45:19.374768Z","shell.execute_reply.started":"2024-07-24T10:30:41.994909Z","shell.execute_reply":"2024-07-24T10:45:19.373978Z"},"trusted":true},"execution_count":17,"outputs":[{"name":"stderr","text":"\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m The `run_name` is currently set to the same value as `TrainingArguments.output_dir`. If this was not intended, please specify a different run name by setting the `TrainingArguments.run_name` parameter.\n\u001b[34m\u001b[1mwandb\u001b[0m: Logging into wandb.ai. (Learn how to deploy a W&B server locally: https://wandb.me/wandb-server)\n\u001b[34m\u001b[1mwandb\u001b[0m: You can find your API key in your browser here: https://wandb.ai/authorize\n\u001b[34m\u001b[1mwandb\u001b[0m: Paste an API key from your profile and hit enter, or press ctrl+c to quit:","output_type":"stream"},{"output_type":"stream","name":"stdin","text":"  ¬∑¬∑¬∑¬∑¬∑¬∑¬∑¬∑¬∑¬∑¬∑¬∑¬∑¬∑¬∑¬∑¬∑¬∑¬∑¬∑¬∑¬∑¬∑¬∑¬∑¬∑¬∑¬∑¬∑¬∑¬∑¬∑¬∑¬∑¬∑¬∑¬∑¬∑¬∑¬∑\n"},{"name":"stderr","text":"\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"wandb version 0.17.5 is available!  To upgrade, please run:\n $ pip install wandb --upgrade"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Tracking run with wandb version 0.17.4"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Run data is saved locally in <code>/kaggle/working/wandb/run-20240724_103054-aku10nmi</code>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Syncing run <strong><a href='https://wandb.ai/tranchienbn03-hanoi-university-of-science-and-technology/huggingface/runs/aku10nmi' target=\"_blank\">distilbert-base-uncased-lora-text-classification</a></strong> to <a href='https://wandb.ai/tranchienbn03-hanoi-university-of-science-and-technology/huggingface' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View project at <a href='https://wandb.ai/tranchienbn03-hanoi-university-of-science-and-technology/huggingface' target=\"_blank\">https://wandb.ai/tranchienbn03-hanoi-university-of-science-and-technology/huggingface</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View run at <a href='https://wandb.ai/tranchienbn03-hanoi-university-of-science-and-technology/huggingface/runs/aku10nmi' target=\"_blank\">https://wandb.ai/tranchienbn03-hanoi-university-of-science-and-technology/huggingface/runs/aku10nmi</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='40000' max='40000' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [40000/40000 14:06, Epoch 10/10]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Epoch</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n      <th>Accuracy</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>1</td>\n      <td>0.666600</td>\n      <td>0.429088</td>\n      <td>{'accuracy': 0.878}</td>\n    </tr>\n    <tr>\n      <td>2</td>\n      <td>0.567200</td>\n      <td>0.561128</td>\n      <td>{'accuracy': 0.8925}</td>\n    </tr>\n    <tr>\n      <td>3</td>\n      <td>0.536000</td>\n      <td>0.513265</td>\n      <td>{'accuracy': 0.9035}</td>\n    </tr>\n    <tr>\n      <td>4</td>\n      <td>0.480400</td>\n      <td>0.435311</td>\n      <td>{'accuracy': 0.9085}</td>\n    </tr>\n    <tr>\n      <td>5</td>\n      <td>0.496000</td>\n      <td>0.386505</td>\n      <td>{'accuracy': 0.902}</td>\n    </tr>\n    <tr>\n      <td>6</td>\n      <td>0.457700</td>\n      <td>0.407456</td>\n      <td>{'accuracy': 0.91}</td>\n    </tr>\n    <tr>\n      <td>7</td>\n      <td>0.442100</td>\n      <td>0.360833</td>\n      <td>{'accuracy': 0.912}</td>\n    </tr>\n    <tr>\n      <td>8</td>\n      <td>0.320700</td>\n      <td>0.327478</td>\n      <td>{'accuracy': 0.916}</td>\n    </tr>\n    <tr>\n      <td>9</td>\n      <td>0.296600</td>\n      <td>0.302460</td>\n      <td>{'accuracy': 0.919}</td>\n    </tr>\n    <tr>\n      <td>10</td>\n      <td>0.244700</td>\n      <td>0.309313</td>\n      <td>{'accuracy': 0.919}</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"name":"stderr","text":"Trainer is attempting to log a value of \"{'accuracy': 0.878}\" of type <class 'dict'> for key \"eval/accuracy\" as a scalar. This invocation of Tensorboard's writer.add_scalar() is incorrect so we dropped this attribute.\nTrainer is attempting to log a value of \"{'accuracy': 0.8925}\" of type <class 'dict'> for key \"eval/accuracy\" as a scalar. This invocation of Tensorboard's writer.add_scalar() is incorrect so we dropped this attribute.\nTrainer is attempting to log a value of \"{'accuracy': 0.9035}\" of type <class 'dict'> for key \"eval/accuracy\" as a scalar. This invocation of Tensorboard's writer.add_scalar() is incorrect so we dropped this attribute.\nTrainer is attempting to log a value of \"{'accuracy': 0.9085}\" of type <class 'dict'> for key \"eval/accuracy\" as a scalar. This invocation of Tensorboard's writer.add_scalar() is incorrect so we dropped this attribute.\nTrainer is attempting to log a value of \"{'accuracy': 0.902}\" of type <class 'dict'> for key \"eval/accuracy\" as a scalar. This invocation of Tensorboard's writer.add_scalar() is incorrect so we dropped this attribute.\nTrainer is attempting to log a value of \"{'accuracy': 0.91}\" of type <class 'dict'> for key \"eval/accuracy\" as a scalar. This invocation of Tensorboard's writer.add_scalar() is incorrect so we dropped this attribute.\nTrainer is attempting to log a value of \"{'accuracy': 0.912}\" of type <class 'dict'> for key \"eval/accuracy\" as a scalar. This invocation of Tensorboard's writer.add_scalar() is incorrect so we dropped this attribute.\nTrainer is attempting to log a value of \"{'accuracy': 0.916}\" of type <class 'dict'> for key \"eval/accuracy\" as a scalar. This invocation of Tensorboard's writer.add_scalar() is incorrect so we dropped this attribute.\nTrainer is attempting to log a value of \"{'accuracy': 0.919}\" of type <class 'dict'> for key \"eval/accuracy\" as a scalar. This invocation of Tensorboard's writer.add_scalar() is incorrect so we dropped this attribute.\nTrainer is attempting to log a value of \"{'accuracy': 0.919}\" of type <class 'dict'> for key \"eval/accuracy\" as a scalar. This invocation of Tensorboard's writer.add_scalar() is incorrect so we dropped this attribute.\n","output_type":"stream"},{"execution_count":17,"output_type":"execute_result","data":{"text/plain":"TrainOutput(global_step=40000, training_loss=0.45008111057281497, metrics={'train_runtime': 875.7671, 'train_samples_per_second': 182.697, 'train_steps_per_second': 45.674, 'total_flos': 1471518910155168.0, 'train_loss': 0.45008111057281497, 'epoch': 10.0})"},"metadata":{}}]},{"cell_type":"code","source":"test_dataset = tokenized_dataset['test']\n\n# ƒê√°nh gi√° m√¥ h√¨nh tr√™n t·∫≠p d·ªØ li·ªáu ki·ªÉm tra\ntest_results = trainer.evaluate(eval_dataset=test_dataset)\n\n# Hi·ªÉn th·ªã k·∫øt qu·∫£ ƒë√°nh gi√°\nprint(\"Test results:\")\nfor key, value in test_results.items():\n    print(f\"{key}: {value}\")\n","metadata":{"execution":{"iopub.status.busy":"2024-07-24T10:45:54.541826Z","iopub.execute_input":"2024-07-24T10:45:54.542170Z","iopub.status.idle":"2024-07-24T10:45:58.475834Z","shell.execute_reply.started":"2024-07-24T10:45:54.542145Z","shell.execute_reply":"2024-07-24T10:45:58.474893Z"},"trusted":true},"execution_count":18,"outputs":[{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='500' max='500' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [500/500 00:03]\n    </div>\n    "},"metadata":{}},{"name":"stderr","text":"Trainer is attempting to log a value of \"{'accuracy': 0.916}\" of type <class 'dict'> for key \"eval/accuracy\" as a scalar. This invocation of Tensorboard's writer.add_scalar() is incorrect so we dropped this attribute.\n","output_type":"stream"},{"name":"stdout","text":"Test results:\neval_loss: 0.2890542149543762\neval_accuracy: {'accuracy': 0.916}\neval_runtime: 3.9139\neval_samples_per_second: 511.003\neval_steps_per_second: 127.751\nepoch: 10.0\n","output_type":"stream"}]},{"cell_type":"code","source":"model.to('cpu')\n\nprint(\"Trained model predictions: \")\nfor text in text_list:\n  # tokenize text\n  inputs = tokenizer.encode(text, return_tensors='pt').to('cpu')\n  # compute logits\n  logits = model(inputs).logits\n  # convert logits to label\n  predictions = torch.argmax(logits, dim=1).item()\n\n  print(text + \" - \" + id2label[predictions])","metadata":{"execution":{"iopub.status.busy":"2024-07-24T10:46:05.207915Z","iopub.execute_input":"2024-07-24T10:46:05.208630Z","iopub.status.idle":"2024-07-24T10:46:05.629209Z","shell.execute_reply.started":"2024-07-24T10:46:05.208590Z","shell.execute_reply":"2024-07-24T10:46:05.628152Z"},"trusted":true},"execution_count":19,"outputs":[{"name":"stdout","text":"Trained model predictions: \ni am sad. - sadness\ntoday, i have a boy friend, i'm happy - joy\nthis is the most yummy dishes - joy\nwowwww - joy\nexciting film! - joy\n","output_type":"stream"}]},{"cell_type":"code","source":"model.save_pretrained('lora_text_classification_model')\n# tokenizer.save_pretrained('tokenizer')","metadata":{"execution":{"iopub.status.busy":"2024-07-24T10:46:53.806890Z","iopub.execute_input":"2024-07-24T10:46:53.807306Z","iopub.status.idle":"2024-07-24T10:46:54.281196Z","shell.execute_reply.started":"2024-07-24T10:46:53.807277Z","shell.execute_reply":"2024-07-24T10:46:54.280034Z"},"trusted":true},"execution_count":20,"outputs":[]},{"cell_type":"code","source":"# reload and merge\nbase_model = AutoModelForSequenceClassification.from_pretrained(\n    model_checkpoint,\n    num_labels=6, \n    id2label=id2label, \n    label2id=label2id,\n)\nmodel = PeftModel.from_pretrained(base_model, '/kaggle/working/lora_text_classification_model')\nmodel = model.merge_and_unload()","metadata":{"execution":{"iopub.status.busy":"2024-07-24T10:58:33.951748Z","iopub.execute_input":"2024-07-24T10:58:33.952750Z","iopub.status.idle":"2024-07-24T10:58:34.836362Z","shell.execute_reply.started":"2024-07-24T10:58:33.952707Z","shell.execute_reply":"2024-07-24T10:58:34.832259Z"},"trusted":true},"execution_count":28,"outputs":[{"name":"stderr","text":"Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","output_type":"stream"}]},{"cell_type":"code","source":"# reload tokenize to save it\ntokenizer = AutoTokenizer.from_pretrained(model_checkpoint, trust_remote_code=True)\ntokenizer.pad_token = tokenizer.eos_token\ntokenizer.padding_side = \"right\"","metadata":{"execution":{"iopub.status.busy":"2024-07-24T11:00:55.226564Z","iopub.execute_input":"2024-07-24T11:00:55.226948Z","iopub.status.idle":"2024-07-24T11:00:55.507426Z","shell.execute_reply.started":"2024-07-24T11:00:55.226917Z","shell.execute_reply":"2024-07-24T11:00:55.506226Z"},"trusted":true},"execution_count":31,"outputs":[]},{"cell_type":"code","source":"model.save_pretrained(\"models/finetune_model.pt\")\ntokenizer.save_pretrained(\"models/tokenizer/\")","metadata":{"execution":{"iopub.status.busy":"2024-07-24T11:01:22.124337Z","iopub.execute_input":"2024-07-24T11:01:22.125013Z","iopub.status.idle":"2024-07-24T11:01:22.580479Z","shell.execute_reply.started":"2024-07-24T11:01:22.124983Z","shell.execute_reply":"2024-07-24T11:01:22.578724Z"},"trusted":true},"execution_count":32,"outputs":[{"execution_count":32,"output_type":"execute_result","data":{"text/plain":"('models/tokenizer/tokenizer_config.json',\n 'models/tokenizer/special_tokens_map.json',\n 'models/tokenizer/vocab.txt',\n 'models/tokenizer/added_tokens.json',\n 'models/tokenizer/tokenizer.json')"},"metadata":{}}]}]}